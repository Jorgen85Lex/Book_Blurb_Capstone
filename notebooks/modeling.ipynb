{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23816ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lexil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lexil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508a0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text_description):\n",
    "    text_description = text_description.lower()\n",
    "    text_description = text_description.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text_description)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd53ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>genre</th>\n",
       "      <th>published_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>processed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the silver chair</td>\n",
       "      <td>two english children undergo hairraising adven...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>1998</td>\n",
       "      <td>clive staples lewis</td>\n",
       "      <td>two english child undergo hairraising adventur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a game of thrones</td>\n",
       "      <td>fantasyroman</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2011</td>\n",
       "      <td>george r r martin</td>\n",
       "      <td>fantasyroman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fablehaven</td>\n",
       "      <td>when kendra and seth go to stay at their grand...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2007</td>\n",
       "      <td>brandon mull</td>\n",
       "      <td>kendra seth go stay grandparent estate discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a wizard of earthsea</td>\n",
       "      <td>originally published in 1968 ursula k le guins...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2012</td>\n",
       "      <td>ursula k le guin</td>\n",
       "      <td>originally published 1968 ursula k le guins wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lodestar</td>\n",
       "      <td>betrayed by one of their closest allies sophie...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2017</td>\n",
       "      <td>shannon messenger</td>\n",
       "      <td>betrayed one closest ally sophies whole world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>out of the everywhere</td>\n",
       "      <td>topics include astronomy humanity radiation ma...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>1990</td>\n",
       "      <td>isaac asimov</td>\n",
       "      <td>topic include astronomy humanity radiation mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>quantum shorts</td>\n",
       "      <td>this book presents winning and shortlisted sto...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2019</td>\n",
       "      <td>michael brooks jenny hogan puah xin yi</td>\n",
       "      <td>book present winning shortlisted story past ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>novel science</td>\n",
       "      <td>novel science is the first indepth study of th...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2013</td>\n",
       "      <td>adelene buckland</td>\n",
       "      <td>novel science first indepth study shocking gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>fantastic voyages</td>\n",
       "      <td>by revealing the facts behind the fiction of s...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2006</td>\n",
       "      <td>leroy w dubeck suzanne e moshier judith e boss</td>\n",
       "      <td>revealing fact behind fiction finest film scif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>holy scifi</td>\n",
       "      <td>can a computer have a soul are religion and sc...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2014</td>\n",
       "      <td>paul j nahin</td>\n",
       "      <td>computer soul religion science mutually exclus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "0         the silver chair  two english children undergo hairraising adven...   \n",
       "1        a game of thrones                                       fantasyroman   \n",
       "2               fablehaven  when kendra and seth go to stay at their grand...   \n",
       "3     a wizard of earthsea  originally published in 1968 ursula k le guins...   \n",
       "4                 lodestar  betrayed by one of their closest allies sophie...   \n",
       "..                     ...                                                ...   \n",
       "776  out of the everywhere  topics include astronomy humanity radiation ma...   \n",
       "777         quantum shorts  this book presents winning and shortlisted sto...   \n",
       "778          novel science  novel science is the first indepth study of th...   \n",
       "779      fantastic voyages  by revealing the facts behind the fiction of s...   \n",
       "780             holy scifi  can a computer have a soul are religion and sc...   \n",
       "\n",
       "               genre  published_date  \\\n",
       "0            fantasy            1998   \n",
       "1            fantasy            2011   \n",
       "2            fantasy            2007   \n",
       "3            fantasy            2012   \n",
       "4            fantasy            2017   \n",
       "..               ...             ...   \n",
       "776  science fiction            1990   \n",
       "777  science fiction            2019   \n",
       "778  science fiction            2013   \n",
       "779  science fiction            2006   \n",
       "780  science fiction            2014   \n",
       "\n",
       "                                            authors  \\\n",
       "0                               clive staples lewis   \n",
       "1                                 george r r martin   \n",
       "2                                      brandon mull   \n",
       "3                                  ursula k le guin   \n",
       "4                                 shannon messenger   \n",
       "..                                              ...   \n",
       "776                                    isaac asimov   \n",
       "777          michael brooks jenny hogan puah xin yi   \n",
       "778                                adelene buckland   \n",
       "779  leroy w dubeck suzanne e moshier judith e boss   \n",
       "780                                    paul j nahin   \n",
       "\n",
       "                                 processed_description  \n",
       "0    two english child undergo hairraising adventur...  \n",
       "1                                         fantasyroman  \n",
       "2    kendra seth go stay grandparent estate discove...  \n",
       "3    originally published 1968 ursula k le guins wi...  \n",
       "4    betrayed one closest ally sophies whole world ...  \n",
       "..                                                 ...  \n",
       "776  topic include astronomy humanity radiation mag...  \n",
       "777  book present winning shortlisted story past ed...  \n",
       "778  novel science first indepth study shocking gro...  \n",
       "779  revealing fact behind fiction finest film scif...  \n",
       "780  computer soul religion science mutually exclus...  \n",
       "\n",
       "[781 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbooksprocessed = pd.read_csv('../data/allbooksprocessed.csv')\n",
    "allbooksprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3c60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allbooksprocessed['processed_description']\n",
    "y = allbooksprocessed['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75566618",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf50a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d82816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           fantasy       0.85      0.63      0.72        27\n",
      "historical fiction       0.78      0.28      0.41        25\n",
      "           mystery       0.72      0.70      0.71        33\n",
      "           romance       0.52      0.81      0.63        37\n",
      "   science fiction       0.84      0.91      0.88        35\n",
      "\n",
      "          accuracy                           0.69       157\n",
      "         macro avg       0.74      0.67      0.67       157\n",
      "      weighted avg       0.73      0.69      0.68       157\n",
      "\n",
      "Accuracy: 0.6942675159235668\n",
      "Confusion Matrix:\n",
      " [[17  1  1  7  1]\n",
      " [ 2  7  1 14  1]\n",
      " [ 0  0 23  6  4]\n",
      " [ 0  1  6 30  0]\n",
      " [ 1  0  1  1 32]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48714e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in fantasy: ['world', 'narnia', 'six', 'edition', 'moomins', 'prince', 'magical', 'wizard', 'oz', 'adventure']\n",
      "Top words in historical fiction: ['soon', 'tribe', 'love', 'author', 'woman', 'london', 'young', 'life', 'family', 'war']\n",
      "Top words in mystery: ['miss', 'wife', 'nancy', 'crime', 'marple', 'killer', 'death', 'fantasyroman', 'mystery', 'murder']\n",
      "Top words in romance: ['town', 'beach', 'shes', 'arrangement', 'york', 'price', 'romantic', 'bestselling', 'heart', 'love']\n",
      "Top words in science fiction: ['form', 'explores', 'literature', 'space', 'literary', 'study', 'work', 'future', 'fiction', 'science']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefs = model.coef_\n",
    "\n",
    "for i, genre in enumerate(model.classes_):\n",
    "    top_features = np.argsort(coefs[i])[-10:]  # Top 10 features\n",
    "    print(f\"Top words in {genre}: {[feature_names[j] for j in top_features]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4b2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=5000)\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a94a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train_count, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082b9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6369426751592356\n",
      "[[11  0  1 11  4]\n",
      " [ 1  2  0 19  3]\n",
      " [ 0  0 22  7  4]\n",
      " [ 0  1  3 32  1]\n",
      " [ 1  0  1  0 33]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bcd1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in fantasy: ['world', 'narnia', 'six', 'edition', 'moomins', 'prince', 'magical', 'wizard', 'oz', 'adventure']\n",
      "Top words in historical fiction: ['soon', 'tribe', 'love', 'author', 'woman', 'london', 'young', 'life', 'family', 'war']\n",
      "Top words in mystery: ['miss', 'wife', 'nancy', 'crime', 'marple', 'killer', 'death', 'fantasyroman', 'mystery', 'murder']\n",
      "Top words in romance: ['town', 'beach', 'shes', 'arrangement', 'york', 'price', 'romantic', 'bestselling', 'heart', 'love']\n",
      "Top words in science fiction: ['form', 'explores', 'literature', 'space', 'literary', 'study', 'work', 'future', 'fiction', 'science']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefs = model.coef_\n",
    "\n",
    "for i, genre in enumerate(model.classes_):\n",
    "    top_features = np.argsort(coefs[i])[-10:] \n",
    "    print(f\"Top words in {genre}: {[feature_names[j] for j in top_features]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9efd1114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>genre</th>\n",
       "      <th>published_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>processed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the silver chair</td>\n",
       "      <td>two english children undergo hairraising adven...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>1998</td>\n",
       "      <td>clive staples lewis</td>\n",
       "      <td>two english child undergo hairraising adventur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a game of thrones</td>\n",
       "      <td>fantasyroman</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2011</td>\n",
       "      <td>george r r martin</td>\n",
       "      <td>fantasyroman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fablehaven</td>\n",
       "      <td>when kendra and seth go to stay at their grand...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2007</td>\n",
       "      <td>brandon mull</td>\n",
       "      <td>kendra seth go stay grandparent estate discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a wizard of earthsea</td>\n",
       "      <td>originally published in 1968 ursula k le guins...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2012</td>\n",
       "      <td>ursula k le guin</td>\n",
       "      <td>originally published 1968 ursula k le guins wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lodestar</td>\n",
       "      <td>betrayed by one of their closest allies sophie...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>2017</td>\n",
       "      <td>shannon messenger</td>\n",
       "      <td>betrayed one closest ally sophies whole world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>out of the everywhere</td>\n",
       "      <td>topics include astronomy humanity radiation ma...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>1990</td>\n",
       "      <td>isaac asimov</td>\n",
       "      <td>topic include astronomy humanity radiation mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>quantum shorts</td>\n",
       "      <td>this book presents winning and shortlisted sto...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2019</td>\n",
       "      <td>michael brooks jenny hogan puah xin yi</td>\n",
       "      <td>book present winning shortlisted story past ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>novel science</td>\n",
       "      <td>novel science is the first indepth study of th...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2013</td>\n",
       "      <td>adelene buckland</td>\n",
       "      <td>novel science first indepth study shocking gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>fantastic voyages</td>\n",
       "      <td>by revealing the facts behind the fiction of s...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2006</td>\n",
       "      <td>leroy w dubeck suzanne e moshier judith e boss</td>\n",
       "      <td>revealing fact behind fiction finest film scif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>holy scifi</td>\n",
       "      <td>can a computer have a soul are religion and sc...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>2014</td>\n",
       "      <td>paul j nahin</td>\n",
       "      <td>computer soul religion science mutually exclus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "0         the silver chair  two english children undergo hairraising adven...   \n",
       "1        a game of thrones                                       fantasyroman   \n",
       "2               fablehaven  when kendra and seth go to stay at their grand...   \n",
       "3     a wizard of earthsea  originally published in 1968 ursula k le guins...   \n",
       "4                 lodestar  betrayed by one of their closest allies sophie...   \n",
       "..                     ...                                                ...   \n",
       "776  out of the everywhere  topics include astronomy humanity radiation ma...   \n",
       "777         quantum shorts  this book presents winning and shortlisted sto...   \n",
       "778          novel science  novel science is the first indepth study of th...   \n",
       "779      fantastic voyages  by revealing the facts behind the fiction of s...   \n",
       "780             holy scifi  can a computer have a soul are religion and sc...   \n",
       "\n",
       "               genre  published_date  \\\n",
       "0            fantasy            1998   \n",
       "1            fantasy            2011   \n",
       "2            fantasy            2007   \n",
       "3            fantasy            2012   \n",
       "4            fantasy            2017   \n",
       "..               ...             ...   \n",
       "776  science fiction            1990   \n",
       "777  science fiction            2019   \n",
       "778  science fiction            2013   \n",
       "779  science fiction            2006   \n",
       "780  science fiction            2014   \n",
       "\n",
       "                                            authors  \\\n",
       "0                               clive staples lewis   \n",
       "1                                 george r r martin   \n",
       "2                                      brandon mull   \n",
       "3                                  ursula k le guin   \n",
       "4                                 shannon messenger   \n",
       "..                                              ...   \n",
       "776                                    isaac asimov   \n",
       "777          michael brooks jenny hogan puah xin yi   \n",
       "778                                adelene buckland   \n",
       "779  leroy w dubeck suzanne e moshier judith e boss   \n",
       "780                                    paul j nahin   \n",
       "\n",
       "                                 processed_description  \n",
       "0    two english child undergo hairraising adventur...  \n",
       "1                                         fantasyroman  \n",
       "2    kendra seth go stay grandparent estate discove...  \n",
       "3    originally published 1968 ursula k le guins wi...  \n",
       "4    betrayed one closest ally sophies whole world ...  \n",
       "..                                                 ...  \n",
       "776  topic include astronomy humanity radiation mag...  \n",
       "777  book present winning shortlisted story past ed...  \n",
       "778  novel science first indepth study shocking gro...  \n",
       "779  revealing fact behind fiction finest film scif...  \n",
       "780  computer soul religion science mutually exclus...  \n",
       "\n",
       "[781 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbooksprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1805aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "      <th>published_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the silver chair</td>\n",
       "      <td>two english children undergo hairraising adven...</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>clive staples lewis</td>\n",
       "      <td>two english child undergo hairraising adventur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a game of thrones</td>\n",
       "      <td>fantasyroman</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>george r r martin</td>\n",
       "      <td>fantasyroman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fablehaven</td>\n",
       "      <td>when kendra and seth go to stay at their grand...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>brandon mull</td>\n",
       "      <td>kendra seth go stay grandparent estate discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a wizard of earthsea</td>\n",
       "      <td>originally published in 1968 ursula k le guins...</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>ursula k le guin</td>\n",
       "      <td>originally published 1968 ursula k le guins wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lodestar</td>\n",
       "      <td>betrayed by one of their closest allies sophie...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>shannon messenger</td>\n",
       "      <td>betrayed one closest ally sophies whole world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>out of the everywhere</td>\n",
       "      <td>topics include astronomy humanity radiation ma...</td>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>isaac asimov</td>\n",
       "      <td>topic include astronomy humanity radiation mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>quantum shorts</td>\n",
       "      <td>this book presents winning and shortlisted sto...</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>michael brooks jenny hogan puah xin yi</td>\n",
       "      <td>book present winning shortlisted story past ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>novel science</td>\n",
       "      <td>novel science is the first indepth study of th...</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>adelene buckland</td>\n",
       "      <td>novel science first indepth study shocking gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>fantastic voyages</td>\n",
       "      <td>by revealing the facts behind the fiction of s...</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>leroy w dubeck suzanne e moshier judith e boss</td>\n",
       "      <td>revealing fact behind fiction finest film scif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>holy scifi</td>\n",
       "      <td>can a computer have a soul are religion and sc...</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>paul j nahin</td>\n",
       "      <td>computer soul religion science mutually exclus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "0         the silver chair  two english children undergo hairraising adven...   \n",
       "1        a game of thrones                                       fantasyroman   \n",
       "2               fablehaven  when kendra and seth go to stay at their grand...   \n",
       "3     a wizard of earthsea  originally published in 1968 ursula k le guins...   \n",
       "4                 lodestar  betrayed by one of their closest allies sophie...   \n",
       "..                     ...                                                ...   \n",
       "776  out of the everywhere  topics include astronomy humanity radiation ma...   \n",
       "777         quantum shorts  this book presents winning and shortlisted sto...   \n",
       "778          novel science  novel science is the first indepth study of th...   \n",
       "779      fantastic voyages  by revealing the facts behind the fiction of s...   \n",
       "780             holy scifi  can a computer have a soul are religion and sc...   \n",
       "\n",
       "     label  published_date                                         authors  \\\n",
       "0        0            1998                             clive staples lewis   \n",
       "1        0            2011                               george r r martin   \n",
       "2        0            2007                                    brandon mull   \n",
       "3        0            2012                                ursula k le guin   \n",
       "4        0            2017                               shannon messenger   \n",
       "..     ...             ...                                             ...   \n",
       "776      4            1990                                    isaac asimov   \n",
       "777      4            2019          michael brooks jenny hogan puah xin yi   \n",
       "778      4            2013                                adelene buckland   \n",
       "779      4            2006  leroy w dubeck suzanne e moshier judith e boss   \n",
       "780      4            2014                                    paul j nahin   \n",
       "\n",
       "                                                  text  \n",
       "0    two english child undergo hairraising adventur...  \n",
       "1                                         fantasyroman  \n",
       "2    kendra seth go stay grandparent estate discove...  \n",
       "3    originally published 1968 ursula k le guins wi...  \n",
       "4    betrayed one closest ally sophies whole world ...  \n",
       "..                                                 ...  \n",
       "776  topic include astronomy humanity radiation mag...  \n",
       "777  book present winning shortlisted story past ed...  \n",
       "778  novel science first indepth study shocking gro...  \n",
       "779  revealing fact behind fiction finest film scif...  \n",
       "780  computer soul religion science mutually exclus...  \n",
       "\n",
       "[781 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbooksprocessed = allbooksprocessed.rename(columns={\"processed_description\": \"text\", \"genre\": \"label\"})\n",
    "allbooksprocessed['label'] = LabelEncoder().fit_transform(allbooksprocessed['label'])\n",
    "allbooksprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "926d5634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = allbooksprocessed['label'].nunique()\n",
    "num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52cfaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(allbooksprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68eac648",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f73aff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'description', 'label', 'published_date', 'authors', 'text'],\n",
       "    num_rows: 624\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e757e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0252fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1403805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f03f1b06247019cc9a908a177827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494058c3719a4867a36101b6f29eb397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda df: tokenizer(df['text'], padding=\"max_length\", truncation=True), batched=True)\n",
    "test_dataset = test_dataset.map(lambda df: tokenizer(df['text'], padding=\"max_length\", truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97613f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f507b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e04935cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TrainingArguments(num_train_epochs = 5,\n",
    "                            weight_decay = 0.01,\n",
    "                            report_to = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3af7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_object = Trainer(\n",
    "    model = model,\n",
    "    args = training,\n",
    "    train_dataset = train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21065e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "c:\\Users\\lexil\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_object\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2245\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2561\u001b[0m ):\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3751\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:920\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    918\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 920\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    921\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    922\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    923\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    924\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    925\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    926\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    927\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    928\u001b[0m )\n\u001b[0;32m    929\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    930\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:739\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    735\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    736\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    737\u001b[0m         )\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    740\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    741\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    742\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    743\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    744\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    745\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    746\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:544\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    536\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    537\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    538\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m         output_attentions,\n\u001b[0;32m    542\u001b[0m     )\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    545\u001b[0m         hidden_state,\n\u001b[0;32m    546\u001b[0m         attn_mask,\n\u001b[0;32m    547\u001b[0m         head_mask[i],\n\u001b[0;32m    548\u001b[0m         output_attentions,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    551\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:470\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    471\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    472\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    473\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    474\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    475\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    476\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    477\u001b[0m )\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    479\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:396\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    393\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    394\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 396\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    397\u001b[0m     q,\n\u001b[0;32m    398\u001b[0m     k,\n\u001b[0;32m    399\u001b[0m     v,\n\u001b[0;32m    400\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    401\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    402\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    403\u001b[0m )\n\u001b[0;32m    405\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[0;32m    406\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_object.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexil\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted = training_object.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = predicted.predictions\n",
    "true_labels = predicted.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predicted.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  3  1  1  1]\n",
      " [ 1 12  2  5  0]\n",
      " [ 1  4 25  2  2]\n",
      " [ 1  3  1 24  0]\n",
      " [ 3  1  1  0 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        34\n",
      "           1       0.52      0.60      0.56        20\n",
      "           2       0.83      0.74      0.78        34\n",
      "           3       0.75      0.83      0.79        29\n",
      "           4       0.92      0.88      0.90        40\n",
      "\n",
      "    accuracy                           0.79       157\n",
      "   macro avg       0.77      0.77      0.77       157\n",
      "weighted avg       0.80      0.79      0.79       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, preds))\n",
    "print(classification_report(true_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e9f9c",
   "metadata": {},
   "source": [
    "Fantasy - pretty strong, pretty distinct words. \n",
    "Historical fiction - worst performancce at 52% possible crossover with things like love, war, family  \n",
    "Mystery - good, maybe crossover? (crime/love could be getting things mixed with romance or historical fiction perhaps.)\n",
    "Romance - Good \n",
    "Sci-Fi  - best - likely because the vocabulary is pretty distinct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec673236",
   "metadata": {},
   "source": [
    "Next steps - try getting more books? \n",
    "attempt to look at bi-grams \n",
    "topic modeling --  pi - LDA -vis\n",
    "LoRA ?\n",
    "Jaccard similarity between genres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "allbooksprocessed['processed_text'] = allbooksprocessed['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71201258",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = allbooksprocessed['processed_text'].tolist()\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b5a732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.009*\"book\" + 0.007*\"time\" + 0.006*\"story\" + 0.005*\"love\" + 0.005*\"life\" + 0.004*\"new\" + 0.004*\"fiction\" + 0.003*\"one\" + 0.003*\"travel\" + 0.003*\"first\"\n",
      "Topic 1: 0.004*\"life\" + 0.003*\"man\" + 0.003*\"story\" + 0.003*\"novel\" + 0.003*\"time\" + 0.003*\"come\" + 0.003*\"find\" + 0.003*\"young\" + 0.003*\"world\" + 0.002*\"two\"\n",
      "Topic 2: 0.004*\"new\" + 0.004*\"life\" + 0.004*\"fiction\" + 0.004*\"time\" + 0.004*\"one\" + 0.004*\"book\" + 0.003*\"year\" + 0.003*\"author\" + 0.003*\"love\" + 0.003*\"find\"\n",
      "Topic 3: 0.007*\"new\" + 0.007*\"life\" + 0.006*\"one\" + 0.005*\"time\" + 0.005*\"love\" + 0.005*\"author\" + 0.005*\"world\" + 0.004*\"york\" + 0.004*\"book\" + 0.004*\"bestselling\"\n",
      "Topic 4: 0.022*\"science\" + 0.017*\"fiction\" + 0.006*\"new\" + 0.006*\"world\" + 0.005*\"book\" + 0.004*\"future\" + 0.004*\"story\" + 0.004*\"life\" + 0.003*\"human\" + 0.003*\"work\"\n"
     ]
    }
   ],
   "source": [
    "num_topics = 5 \n",
    "\n",
    "lda_model = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_topics):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67d8d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el3046819935313920806394267984\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el3046819935313920806394267984_data = {\"mdsDat\": {\"x\": [0.12065875827508395, -0.08495096497242849, -0.0027747168101200023, 0.020757060764931893, -0.053690137257467276], \"y\": [0.019004813784682374, 0.08362736996845765, -0.027515209645419106, 0.009426022712908311, -0.0845429968206292], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.13678485854563, 23.958225054721414, 18.643162369025397, 18.1727066368822, 14.08912108082536]}, \"tinfo\": {\"Term\": [\"science\", \"fiction\", \"travel\", \"climate\", \"future\", \"book\", \"york\", \"scientific\", \"film\", \"sf\", \"technology\", \"bestselling\", \"murder\", \"time\", \"human\", \"father\", \"london\", \"shes\", \"c\", \"text\", \"philosophy\", \"literature\", \"language\", \"study\", \"space\", \"genre\", \"man\", \"good\", \"environmental\", \"begin\", \"sf\", \"ecofeminist\", \"coop\", \"biology\", \"helped\", \"science\", \"prototype\", \"religion\", \"beryl\", \"dune\", \"television\", \"pseudoscience\", \"lawrence\", \"engage\", \"et\", \"gap\", \"computing\", \"scifi\", \"consciousness\", \"honoree\", \"portrayed\", \"engineering\", \"analysis\", \"astronomy\", \"extraterrestrial\", \"agency\", \"nonhuman\", \"godmother\", \"logan\", \"data\", \"le\", \"scientific\", \"mar\", \"genre\", \"fiction\", \"alien\", \"technology\", \"philosophy\", \"de\", \"human\", \"cultural\", \"debate\", \"planet\", \"explore\", \"animal\", \"explores\", \"future\", \"study\", \"popular\", \"field\", \"issue\", \"environmental\", \"film\", \"literature\", \"world\", \"space\", \"u\", \"new\", \"form\", \"work\", \"book\", \"story\", \"well\", \"writer\", \"life\", \"novel\", \"find\", \"author\", \"literary\", \"one\", \"love\", \"family\", \"time\", \"war\", \"like\", \"year\", \"first\", \"tower\", \"hank\", \"reviewer\", \"ida\", \"elzbieta\", \"pastor\", \"juliette\", \"chauveau\", \"irvel\", \"bridgerton\", \"elena\", \"anthony\", \"rachel\", \"lake\", \"swipe\", \"ezra\", \"casino\", \"twobits\", \"amazon\", \"witcher\", \"hussite\", \"purchase\", \"warsaw\", \"grandparent\", \"spell\", \"seventh\", \"frog\", \"determination\", \"meredith\", \"hawke\", \"goodreads\", \"stephen\", \"rose\", \"oz\", \"sunny\", \"icy\", \"shes\", \"york\", \"father\", \"good\", \"bestselling\", \"jonathan\", \"nothing\", \"summer\", \"know\", \"able\", \"one\", \"new\", \"sam\", \"life\", \"author\", \"choice\", \"love\", \"time\", \"world\", \"city\", \"reader\", \"woman\", \"family\", \"novel\", \"must\", \"make\", \"take\", \"book\", \"come\", \"two\", \"home\", \"story\", \"war\", \"young\", \"series\", \"man\", \"first\", \"find\", \"kinkley\", \"piper\", \"julian\", \"rebus\", \"ya\", \"pharaoh\", \"dub\", \"sunset\", \"eleanor\", \"della\", \"china\", \"shakespeare\", \"tariq\", \"charlotte\", \"dexie\", \"faerie\", \"cove\", \"holocaust\", \"astronautics\", \"therapy\", \"psychiatry\", \"kanas\", \"isobelle\", \"expedition\", \"rivalry\", \"brutus\", \"jeffrey\", \"bourne\", \"fragile\", \"average\", \"caesar\", \"c\", \"mary\", \"climate\", \"james\", \"return\", \"henry\", \"year\", \"prince\", \"international\", \"dinner\", \"london\", \"battle\", \"keep\", \"early\", \"town\", \"new\", \"change\", \"break\", \"one\", \"history\", \"life\", \"find\", \"woman\", \"time\", \"author\", \"century\", \"fiction\", \"two\", \"book\", \"love\", \"novel\", \"story\", \"sister\", \"take\", \"war\", \"together\", \"work\", \"family\", \"literary\", \"series\", \"world\", \"secret\", \"friend\", \"must\", \"gansett\", \"beedle\", \"bard\", \"kirsten\", \"mccarthy\", \"joubert\", \"bradford\", \"hoover\", \"cyberspace\", \"stem\", \"rylan\", \"morse\", \"mattie\", \"selection\", \"environmentality\", \"lakota\", \"playful\", \"commits\", \"frontier\", \"colleen\", \"pageturner\", \"deathly\", \"effective\", \"hallows\", \"hermione\", \"penn\", \"rumm\", \"l\\u00e9o\", \"popculture\", \"montverre\", \"ranch\", \"gothic\", \"specific\", \"travel\", \"contains\", \"bibliography\", \"shepard\", \"language\", \"murder\", \"book\", \"silver\", \"australian\", \"illustration\", \"machine\", \"time\", \"dr\", \"story\", \"love\", \"includes\", \"island\", \"first\", \"year\", \"critical\", \"life\", \"mystery\", \"future\", \"family\", \"new\", \"one\", \"past\", \"essay\", \"fiction\", \"tale\", \"u\", \"woman\", \"find\", \"help\", \"determined\", \"author\", \"meet\", \"secret\", \"work\", \"young\", \"world\", \"novel\", \"home\", \"friend\", \"artemis\", \"odysseus\", \"revision\", \"comet\", \"devin\", \"alma\", \"ichimei\", \"mcgee\", \"zack\", \"gabby\", \"derek\", \"anatomy\", \"darryl\", \"code\", \"boyfriend\", \"langdon\", \"legible\", \"bedroom\", \"tina\", \"charmain\", \"dior\", \"winniethepooh\", \"phoebe\", \"yvette\", \"oil\", \"stokes\", \"fantasyroman\", \"dallas\", \"security\", \"deserve\", \"tapestry\", \"joseph\", \"odd\", \"ice\", \"museum\", \"police\", \"text\", \"kingdom\", \"monster\", \"man\", \"melanie\", \"shop\", \"king\", \"single\", \"poor\", \"london\", \"begin\", \"old\", \"historical\", \"come\", \"hand\", \"young\", \"make\", \"life\", \"two\", \"help\", \"novel\", \"find\", \"art\", \"journey\", \"story\", \"work\", \"time\", \"fall\", \"even\", \"lead\", \"world\", \"first\", \"woman\", \"back\", \"family\", \"friend\", \"author\", \"one\", \"love\"], \"Freq\": [231.0, 236.0, 35.0, 30.0, 68.0, 178.0, 63.0, 26.0, 27.0, 15.0, 31.0, 65.0, 38.0, 163.0, 43.0, 38.0, 33.0, 33.0, 15.0, 24.0, 21.0, 36.0, 21.0, 37.0, 37.0, 20.0, 66.0, 25.0, 24.0, 35.0, 15.01084435878395, 8.439185455197377, 8.438591502466007, 6.795927550426423, 6.788868586427823, 210.35726520916216, 5.962055211583473, 8.154490050299009, 5.14507128128508, 5.142375392739601, 5.134930665626812, 5.1330970383567704, 5.130787469582211, 5.12873451956038, 4.307220516789737, 4.3032006202109265, 4.302409889325349, 5.724093922488471, 7.846486871440748, 4.274399479306772, 4.266221913608369, 4.257112504543225, 8.5439415719838, 4.200780235065395, 4.195445793617671, 4.175445673950001, 3.4826952794358252, 3.4818853438930475, 3.4817367165460227, 3.480576365591438, 11.048429267271485, 21.12109940502363, 9.435332694125496, 16.448801138147775, 165.04079378106533, 6.109484076668035, 23.712454496408597, 16.514285122602264, 14.250446811760831, 29.818807378967957, 14.028330756497558, 7.917814081594574, 10.116792976833061, 11.227274168251403, 14.198018654261881, 17.071412784627594, 40.34464062782969, 23.92477895978358, 16.379885921606906, 12.869118721583792, 9.919331248160596, 16.581054984104046, 17.527048564933757, 21.064839222592393, 53.40517912593384, 20.041737551463328, 22.85181197874247, 53.53370448510969, 14.83665533469736, 29.792245946418692, 47.18110670464978, 39.53466500576743, 19.172642758002333, 19.527322160363994, 37.84866310063684, 29.104892895562777, 26.647547345461867, 28.023106245284712, 17.76005839241157, 24.888485357775618, 24.99225062558187, 22.312304728862212, 24.52108968906398, 19.695016113719912, 18.24720539691399, 19.045415522321854, 17.919533865799398, 9.171693783982779, 8.388272196174695, 7.55344118795186, 6.760803818100944, 5.116938033014198, 5.110734755890146, 5.10845282851155, 5.105311398144503, 5.099703936980964, 9.494872213214196, 5.085904138999729, 10.816066251386792, 4.292149680542528, 4.28900586266718, 4.287157135405911, 4.285372870868775, 4.284473576075304, 4.280623321021485, 4.277074953128426, 4.262460218980364, 4.255360830407242, 3.4709033292751146, 3.4653269057194915, 3.463210246971082, 3.4632391623493968, 3.4629126042695257, 3.4628526512832942, 3.461411922662839, 3.4614686923046687, 3.461087486812569, 4.81584670835636, 6.7644492779982555, 11.594009834037005, 10.6194119676293, 7.778754060242774, 5.937883292956953, 23.30675204756745, 40.52180531459111, 25.391515807301925, 17.84323747341217, 38.86973021484987, 7.974221487095841, 12.595895328655812, 11.169340018591583, 27.606444397391034, 6.720495780729614, 52.605339239568735, 66.64483880978932, 6.748526189301108, 64.71608014803262, 45.67645483705968, 10.019575443137656, 48.02010514137467, 48.218788276630825, 42.597290686509524, 16.728747537273133, 25.10391761844796, 30.85714199991031, 31.199137177899992, 32.000058140441446, 19.646379242849413, 20.880326299890672, 20.15083958572266, 40.50642763890146, 22.953708542274377, 22.950881732445698, 19.888665707490023, 31.05506962142847, 21.529335945354894, 21.529456912442157, 19.281139284038748, 19.225152744931798, 20.335331371636173, 19.07266842586226, 6.283352925094757, 5.500092678714905, 5.500138092740821, 5.473364047334516, 4.739490728429005, 4.701239320109324, 3.9735237302632735, 3.9721315839415503, 3.901752642269603, 3.2177936728212324, 3.216112321725379, 3.2156827463256894, 3.2142301167785416, 7.043562663338653, 3.2103073770763317, 3.2088952072977275, 3.207267733750802, 3.2051258431466803, 3.205402662277379, 3.2053855288039648, 3.2032184540581987, 3.2028194299123074, 3.200509094557514, 3.1970638214095963, 3.1900830663441, 3.1727413076932054, 6.28376041276366, 3.1319886186824935, 3.1155761961437576, 2.451334758183872, 4.687452860405499, 10.589632654632323, 7.051877558629134, 15.32004346786155, 7.8071342780646935, 13.12974398924159, 4.736974791393245, 22.962791357752913, 6.189828931541992, 9.228931830163233, 3.980960483434447, 12.20210223310451, 8.956764747137692, 8.606807566265452, 7.044645994284146, 11.950182374251547, 31.841458017377242, 14.531948279140744, 5.439675510055074, 25.55545497744288, 12.899241385010724, 30.032638833582322, 20.03707778618454, 19.20829658450016, 26.066280198044286, 21.794859861945238, 13.016126353494798, 29.912253682264446, 16.225990530963664, 24.88588544210607, 21.44060568851176, 18.750736236331566, 19.817171512764272, 10.995513492727373, 12.453124545663455, 13.646593586808171, 10.827575727727394, 13.883571405425132, 14.946930088029406, 11.254683907245251, 11.933096791992298, 14.988385662304866, 11.817133315379541, 11.664539711172315, 11.354596415680088, 4.662442036124247, 4.659515920407997, 4.657639761493246, 4.65659785342154, 3.9099051174291146, 3.9023091286869596, 3.160620086788604, 3.159049377169498, 3.1574261495884923, 3.156087645823914, 3.155391430696819, 3.153962982376804, 3.0890123487843786, 3.0444499526306323, 2.4082016856396744, 2.407985778462977, 2.4073499288093285, 2.407205454109544, 2.406681280581774, 2.4057997434511944, 2.405487251516981, 2.405017406915314, 2.4041205771981273, 2.4041976437858956, 2.404169272013375, 2.4035929829601956, 2.4030446632430467, 2.4031412480005625, 2.4026728119266094, 2.4025989648307586, 4.665264725662656, 8.416498793698288, 4.637196792691975, 22.984876791105712, 7.79425068852754, 3.8987266390560897, 4.6584462442185055, 11.830662118914812, 18.46942508526284, 59.0003702407932, 6.132005280104231, 5.406233205026292, 5.197889017278745, 7.063265853599156, 50.03312584278635, 8.666718510376429, 39.77626244547242, 35.14916680867067, 9.588516997126689, 7.339074092035242, 22.412964637187482, 20.476754149450688, 6.985412904930288, 32.633072705028646, 11.583599918934471, 17.663470356677443, 21.93222205538066, 28.922357150080508, 23.45870390427438, 13.869849665369362, 8.479425372966016, 28.814919485311467, 12.283786289309893, 12.294471782983097, 16.623770856427225, 16.9001899934546, 12.458843747688968, 9.510723609058289, 18.14355294264124, 10.582954465385905, 12.510510958849252, 13.695704128075835, 13.075708862374151, 15.334428733425025, 12.117429507837063, 10.788919046170147, 10.583843045155053, 3.7509323230315643, 3.7503688418926653, 3.749743895756887, 3.7458756570193046, 3.741675756902625, 3.029982854209643, 3.029743546328394, 3.0297419863030925, 3.0279289248976973, 3.028256842216071, 3.0274381409378477, 3.0238148261725994, 2.990965061393998, 2.979266431658101, 2.9605701524298262, 2.9313808310186484, 2.3138476234681296, 2.310190144148779, 2.3082106280437142, 2.3081488510417754, 2.307953223868968, 2.3070778936723, 2.3067415522172983, 2.3065172205789435, 2.3059886840067976, 2.305403674518738, 2.3045834132152128, 2.3038280489642298, 2.301851652909768, 2.2986883896059305, 4.4631194417400595, 4.466477552204053, 5.9138556107118, 3.6944862395526985, 4.451647639682999, 7.297770279929504, 10.512104444067196, 4.477239854753994, 3.7427440622290873, 18.497754152467447, 3.029644328719219, 3.029342931830971, 8.104874330061806, 5.097166453122064, 4.298070948877506, 10.899160449590298, 11.188180721106848, 10.897056287463643, 12.149160050887348, 14.945194646773754, 7.841354856133584, 14.623588566759615, 12.257762772276095, 22.9247365441136, 13.376295921365715, 10.990358280695347, 15.713787912313395, 14.680262413932498, 7.665254832030194, 7.894849995752146, 16.07631782405654, 12.110832101260023, 15.028353979505267, 7.413444284004859, 9.564786567856931, 8.05125938050009, 13.956821272699033, 11.308418735204251, 11.818012856129277, 8.159286764557647, 10.71064075205666, 9.03806461714175, 10.501178026855538, 10.092664185315456, 9.569768040649658], \"Total\": [231.0, 236.0, 35.0, 30.0, 68.0, 178.0, 63.0, 26.0, 27.0, 15.0, 31.0, 65.0, 38.0, 163.0, 43.0, 38.0, 33.0, 33.0, 15.0, 24.0, 21.0, 36.0, 21.0, 37.0, 37.0, 20.0, 66.0, 25.0, 24.0, 35.0, 15.72520976490377, 9.084262550869397, 9.085443577212576, 7.424113049937882, 7.423310337411761, 231.51247796974258, 6.593337588110489, 9.047476694251657, 5.763566113395445, 5.763274172918664, 5.762699527507506, 5.762764083919945, 5.762250560527861, 5.7618576622251325, 4.93208574957417, 4.931711014751178, 4.931915066511029, 6.570401014099904, 9.038026375798207, 4.929587718764868, 4.927341853556024, 4.927193267107908, 9.894821774307585, 4.9216634912504045, 4.922292842093986, 4.917117161522637, 4.101721771957011, 4.101716403711726, 4.10156055343224, 4.101815907398128, 13.150852037202538, 26.094880418078994, 11.458548218812158, 20.368401307549377, 236.61816799456565, 7.370310611873297, 31.70229367819279, 21.793992398338027, 18.61828241333021, 43.01122858047547, 18.605541338095637, 9.860234089804813, 13.030151805292366, 14.794222776196534, 19.411664437107433, 24.276699180779328, 68.16925336349159, 37.153888649156045, 23.669124239014142, 17.881467034855593, 13.033463409436985, 24.98436493972851, 27.98671080034643, 36.76709845797456, 140.2821054808723, 37.42901722193419, 46.22418600193459, 188.20824494517436, 24.042370542568428, 78.67089468485088, 178.42550164791396, 146.2594864094891, 40.00395555562807, 41.81688966495528, 188.155191331394, 107.68690469248624, 97.33774596489576, 124.1391519137864, 41.73830556646547, 136.60064766437705, 139.17189630478862, 101.10123480222893, 163.86763798603067, 67.77789198056381, 46.47612052886907, 83.78890393596522, 80.76113943448243, 9.86585417185971, 9.042635142403991, 8.214876586290782, 7.39186309335924, 5.738973174292991, 5.738323367335631, 5.738529265839613, 5.73775496317804, 5.7373196438639384, 10.69656361676217, 5.7355674608519935, 12.276332759493275, 4.912324606876056, 4.911906687062473, 4.911934799854217, 4.911919259314906, 4.911521257682561, 4.9112269443794245, 4.911071156665113, 4.90964401546919, 4.9086735425338865, 4.085893334773821, 4.08563441070297, 4.085365595718182, 4.085427211728192, 4.085456492202617, 4.085511821193097, 4.085040022581216, 4.085158943810394, 4.085358887047882, 5.734603602241009, 8.14453926249214, 14.647372471931504, 13.761531390311907, 9.822174016928598, 7.394864265139871, 33.81751412702442, 63.91341731809574, 38.375792750343216, 25.982495533387645, 65.07047194606125, 10.500840667925376, 18.548439845761813, 16.118125987212, 50.62059303897754, 8.83726581753059, 136.60064766437705, 188.20824494517436, 8.97252184375782, 188.155191331394, 124.1391519137864, 15.273020709398855, 139.17189630478862, 163.86763798603067, 140.2821054808723, 34.33424172533876, 63.90365321806411, 91.98658152585348, 101.10123480222893, 107.68690469248624, 47.49009116853064, 54.47873905274375, 51.27877191067332, 178.42550164791396, 70.71482283148876, 71.61029805850747, 54.02414083632813, 146.2594864094891, 67.77789198056381, 70.42951966953734, 54.93896611984237, 66.22315736629872, 80.76113943448243, 97.33774596489576, 6.913517753032458, 6.147503964721562, 6.147568327095545, 6.146292573643361, 5.379715015700126, 5.383052803811197, 4.613524254437028, 4.613685464744452, 4.618953550902451, 3.8465888051529054, 3.8464167170997645, 3.8465510588648613, 3.846568704220282, 8.433250954598, 3.8465011851323374, 3.8462946553431565, 3.846748487722892, 3.84632107322632, 3.8472317122074897, 3.847222611315354, 3.8473619980760194, 3.8473766803132015, 3.847390024582903, 3.8477734249665945, 3.8481371839565544, 3.849390757689431, 7.666168803862292, 3.843598455042472, 3.8433844421682677, 3.079684702975968, 6.106961006759433, 15.687326474177974, 10.893583829346344, 30.244117594048443, 13.90944751457021, 32.83237685506222, 7.034808000512199, 83.78890393596522, 10.851813555522202, 20.526404067795344, 5.438444541375174, 33.70467379669016, 20.53084615630936, 19.753409505953233, 14.22031474783823, 35.4979229639962, 188.20824494517436, 50.83339716134085, 9.460208832005994, 136.60064766437705, 42.182065141997256, 188.155191331394, 97.33774596489576, 91.98658152585348, 163.86763798603067, 124.1391519137864, 45.955785980840794, 236.61816799456565, 71.61029805850747, 178.42550164791396, 139.17189630478862, 107.68690469248624, 146.2594864094891, 33.82308692473582, 51.27877191067332, 67.77789198056381, 35.80287689076998, 78.67089468485088, 101.10123480222893, 41.73830556646547, 54.93896611984237, 140.2821054808723, 57.441767196699026, 55.498245978586816, 47.49009116853064, 5.302144631728182, 5.302331814602245, 5.302303845807477, 5.301979819260714, 4.548327706018279, 4.54864746447362, 3.7942892911759643, 3.794174492818472, 3.794339503767682, 3.794543459491329, 3.794381010332238, 3.794679023584955, 3.800457193598678, 3.7907449730380725, 3.0406323020782464, 3.0404725103671173, 3.040597540587958, 3.0407708437290433, 3.040569009018455, 3.040406024063762, 3.040646427282418, 3.0407643258849872, 3.040604429457183, 3.04083985948663, 3.040808463972051, 3.0405333882287953, 3.040603554252094, 3.0407341630705083, 3.040772843076836, 3.0407101062854607, 6.024581431385497, 11.371362523852248, 6.133727091954797, 35.379025189128065, 11.438949357462679, 5.3144306350544355, 6.748517381087529, 21.113477459074, 38.859189962996375, 178.42550164791396, 9.954054488327316, 8.547454134343548, 8.288202717179844, 12.44034103134791, 163.86763798603067, 17.032664388275546, 146.2594864094891, 139.17189630478862, 20.81060802640657, 13.973958861572084, 80.76113943448243, 83.78890393596522, 13.992660774044058, 188.155191331394, 33.0609410644328, 68.16925336349159, 101.10123480222893, 188.20824494517436, 136.60064766437705, 50.895028350439425, 20.456209004983545, 236.61816799456565, 43.019163431885936, 46.22418600193459, 91.98658152585348, 97.33774596489576, 49.572957000100715, 27.307122623631248, 124.1391519137864, 35.215875797624584, 57.441767196699026, 78.67089468485088, 70.42951966953734, 140.2821054808723, 107.68690469248624, 54.02414083632813, 55.498245978586816, 4.396940313360403, 4.397008677734554, 4.39699283087249, 4.397377796509242, 4.398593898924701, 3.673538057977705, 3.6735847855739245, 3.6738103336847874, 3.673556397020146, 3.674058522779114, 3.6740168086089025, 3.6738892393508134, 3.675900156257554, 3.67570275662968, 3.6773170395081367, 3.677652354709461, 2.9495079280031065, 2.9498506973608394, 2.9500016866486076, 2.9499697263104148, 2.9502940472174624, 2.9501448187967316, 2.9503423310521213, 2.9504776991544372, 2.950256759578552, 2.9502172756000933, 2.949642554833544, 2.9501927460120907, 2.9507652708807277, 2.9505586106189763, 5.873922940615662, 5.9507326735719, 8.980758981525032, 5.152097931022168, 6.700371911957179, 14.298747421310136, 24.19440126665875, 7.405694342787299, 5.903951633111413, 66.22315736629872, 4.4392575523159685, 4.43931471107318, 19.919054670760246, 9.861031926067188, 7.6315525697229525, 33.70467379669016, 35.22774363075121, 36.12563864558387, 46.79449398437558, 70.71482283148876, 21.784616270081187, 70.42951966953734, 54.47873905274375, 188.155191331394, 71.61029805850747, 49.572957000100715, 107.68690469248624, 97.33774596489576, 25.6383470803182, 28.659192630627604, 146.2594864094891, 78.67089468485088, 163.86763798603067, 26.050242526028157, 51.07889466988654, 32.64310152141994, 140.2821054808723, 80.76113943448243, 91.98658152585348, 38.53663493257288, 101.10123480222893, 55.498245978586816, 124.1391519137864, 136.60064766437705, 139.17189630478862], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.4569, -7.0328, -7.0329, -7.2494, -7.2504, -3.8169, -7.3803, -7.0671, -7.5276, -7.5282, -7.5296, -7.53, -7.5304, -7.5308, -7.7054, -7.7063, -7.7065, -7.421, -7.1056, -7.713, -7.715, -7.7171, -7.0205, -7.7304, -7.7317, -7.7365, -7.9179, -7.9181, -7.9182, -7.9185, -6.7634, -6.1154, -6.9212, -6.3654, -4.0595, -7.3558, -5.9997, -6.3615, -6.5089, -5.7705, -6.5246, -7.0966, -6.8515, -6.7473, -6.5126, -6.3283, -5.4682, -5.9908, -6.3696, -6.6109, -6.8712, -6.3574, -6.3019, -6.1181, -5.1878, -6.1679, -6.0367, -5.1854, -6.4686, -5.7714, -5.3117, -5.4885, -6.2122, -6.1939, -5.5321, -5.7948, -5.883, -5.8327, -6.2887, -5.9513, -5.9471, -6.0605, -5.9662, -6.1853, -6.2617, -6.2189, -6.2798, -6.9015, -6.9908, -7.0957, -7.2065, -7.4851, -7.4863, -7.4868, -7.4874, -7.4885, -6.8669, -7.4912, -6.7366, -7.6609, -7.6616, -7.662, -7.6625, -7.6627, -7.6636, -7.6644, -7.6678, -7.6695, -7.8733, -7.8749, -7.8755, -7.8755, -7.8756, -7.8756, -7.876, -7.876, -7.8761, -7.5458, -7.206, -6.6672, -6.755, -7.0663, -7.3363, -5.9689, -5.4158, -5.8833, -6.236, -5.4575, -7.0415, -6.5843, -6.7045, -5.7996, -7.2125, -5.1548, -4.9183, -7.2083, -4.9477, -5.2961, -6.8131, -5.246, -5.2419, -5.3659, -6.3005, -5.8946, -5.6883, -5.6773, -5.6519, -6.1398, -6.0789, -6.1144, -5.4162, -5.9842, -5.9843, -6.1275, -5.6819, -6.0482, -6.0482, -6.1585, -6.1614, -6.1053, -6.1694, -7.0289, -7.1621, -7.1621, -7.1669, -7.3109, -7.319, -7.4872, -7.4875, -7.5054, -7.6981, -7.6987, -7.6988, -7.6992, -6.9147, -7.7005, -7.7009, -7.7014, -7.7021, -7.702, -7.702, -7.7027, -7.7028, -7.7035, -7.7046, -7.7068, -7.7122, -7.0289, -7.7252, -7.7304, -7.9702, -7.3219, -6.507, -6.9135, -6.1377, -6.8118, -6.292, -7.3114, -5.733, -7.0439, -6.6445, -7.4853, -6.3652, -6.6744, -6.7143, -6.9146, -6.3861, -5.4061, -6.1905, -7.1731, -5.626, -6.3097, -5.4645, -5.8692, -5.9115, -5.6062, -5.7852, -6.3006, -5.4686, -6.0802, -5.6525, -5.8015, -5.9356, -5.8803, -6.4693, -6.3449, -6.2533, -6.4847, -6.2361, -6.1623, -6.446, -6.3875, -6.1596, -6.3973, -6.4103, -6.4372, -7.3017, -7.3024, -7.3028, -7.303, -7.4778, -7.4797, -7.6905, -7.691, -7.6915, -7.6919, -7.6922, -7.6926, -7.7134, -7.728, -7.9624, -7.9625, -7.9627, -7.9628, -7.963, -7.9634, -7.9635, -7.9637, -7.9641, -7.9641, -7.9641, -7.9643, -7.9645, -7.9645, -7.9647, -7.9647, -7.3011, -6.7111, -7.3072, -5.7064, -6.7879, -7.4806, -7.3026, -6.3706, -5.9252, -4.7637, -7.0278, -7.1537, -7.193, -6.8864, -4.9286, -6.6818, -5.158, -5.2817, -6.5807, -6.8481, -5.7316, -5.822, -6.8975, -5.3559, -6.3917, -5.9698, -5.7533, -5.4767, -5.686, -6.2116, -6.7036, -5.4804, -6.333, -6.3321, -6.0304, -6.014, -6.3188, -6.5889, -5.943, -6.482, -6.3147, -6.2242, -6.2705, -6.1112, -6.3466, -6.4628, -6.4819, -7.2648, -7.2649, -7.2651, -7.2661, -7.2672, -7.4782, -7.4783, -7.4783, -7.4789, -7.4788, -7.479, -7.4802, -7.4912, -7.4951, -7.5014, -7.5113, -7.7478, -7.7494, -7.7503, -7.7503, -7.7504, -7.7508, -7.7509, -7.751, -7.7512, -7.7515, -7.7519, -7.7522, -7.753, -7.7544, -7.0909, -7.0902, -6.8095, -7.2799, -7.0935, -6.5992, -6.2342, -7.0878, -7.2669, -5.6691, -7.4783, -7.4784, -6.4943, -6.9581, -7.1286, -6.1981, -6.1719, -6.1983, -6.0895, -5.8824, -6.5273, -5.9041, -6.0806, -5.4545, -5.9933, -6.1897, -5.8322, -5.9003, -6.5501, -6.5205, -5.8094, -6.0927, -5.8768, -6.5835, -6.3287, -6.5009, -5.9508, -6.1612, -6.1171, -6.4876, -6.2155, -6.3853, -6.2353, -6.2749, -6.3281], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3343, 1.3072, 1.307, 1.2924, 1.2915, 1.285, 1.2802, 1.2769, 1.2673, 1.2668, 1.2655, 1.2651, 1.2648, 1.2644, 1.2454, 1.2445, 1.2443, 1.2429, 1.2395, 1.2382, 1.2368, 1.2347, 1.234, 1.2225, 1.2211, 1.2173, 1.2172, 1.217, 1.217, 1.2166, 1.2066, 1.1694, 1.1866, 1.1671, 1.0206, 1.1932, 1.0904, 1.1034, 1.1135, 1.0145, 1.0985, 1.1614, 1.1278, 1.1049, 1.0681, 1.0287, 0.8563, 0.9407, 1.0127, 1.0519, 1.1078, 0.9708, 0.9129, 0.8238, 0.4151, 0.7562, 0.6764, 0.1236, 0.8981, 0.4098, 0.0507, 0.0726, 0.6453, 0.6194, -0.2228, 0.0725, 0.0853, -0.1075, 0.5264, -0.3218, -0.3363, -0.1301, -0.5187, 0.145, 0.4459, -0.1006, -0.1248, 1.3559, 1.3537, 1.3449, 1.3396, 1.3141, 1.313, 1.3126, 1.3121, 1.311, 1.3097, 1.3086, 1.3022, 1.2939, 1.2933, 1.2928, 1.2924, 1.2923, 1.2914, 1.2906, 1.2875, 1.286, 1.2657, 1.2642, 1.2636, 1.2636, 1.2635, 1.2635, 1.2632, 1.2632, 1.263, 1.2543, 1.2432, 1.1951, 1.1697, 1.1956, 1.2094, 1.0566, 0.9732, 1.0158, 1.0531, 0.9136, 1.1536, 1.0418, 1.0621, 0.8225, 1.155, 0.4746, 0.3907, 1.144, 0.3616, 0.429, 1.0073, 0.3648, 0.2055, 0.237, 0.7098, 0.4945, 0.3366, 0.2531, 0.2154, 0.5462, 0.4699, 0.4948, -0.0539, 0.3037, 0.291, 0.4296, -0.1208, 0.282, 0.2437, 0.3818, 0.192, 0.0497, -0.2011, 1.5841, 1.5684, 1.5684, 1.5637, 1.553, 1.5443, 1.5304, 1.53, 1.5109, 1.5012, 1.5007, 1.5006, 1.5001, 1.4996, 1.4989, 1.4985, 1.4979, 1.4973, 1.4972, 1.4972, 1.4965, 1.4963, 1.4956, 1.4944, 1.4921, 1.4864, 1.4808, 1.4749, 1.4698, 1.4515, 1.4152, 1.2867, 1.2448, 0.9996, 1.1022, 0.7632, 1.2842, 0.3853, 1.1183, 0.8803, 1.3677, 0.6637, 0.8502, 0.8489, 0.9773, 0.591, -0.0971, 0.4275, 1.1263, 0.0035, 0.4949, -0.1553, 0.0991, 0.1134, -0.1587, -0.06, 0.4182, -0.3885, 0.1951, -0.2902, -0.1907, -0.0683, -0.3191, 0.556, 0.2644, 0.0769, 0.4838, -0.0549, -0.2319, 0.3691, 0.1528, -0.5567, 0.0985, 0.1199, 0.2488, 1.5767, 1.576, 1.5756, 1.5755, 1.554, 1.552, 1.5225, 1.5221, 1.5215, 1.521, 1.5208, 1.5203, 1.498, 1.486, 1.4721, 1.472, 1.4717, 1.4716, 1.4715, 1.4711, 1.4709, 1.4707, 1.4704, 1.4703, 1.4703, 1.4702, 1.4699, 1.4699, 1.4697, 1.4697, 1.4495, 1.4043, 1.4256, 1.274, 1.3216, 1.3955, 1.3346, 1.126, 0.9614, 0.5986, 1.2208, 1.2472, 1.2387, 1.1392, 0.5189, 1.0296, 0.4031, 0.3291, 0.9304, 1.0613, 0.4234, 0.2962, 1.0105, -0.0467, 0.6565, 0.3548, 0.1771, -0.1677, -0.0566, 0.4052, 0.8246, -0.4003, 0.4519, 0.3809, -0.0056, -0.0456, 0.3242, 0.6505, -0.2178, 0.503, 0.181, -0.0429, 0.0214, -0.5083, -0.4793, 0.0943, 0.0482, 1.8009, 1.8007, 1.8005, 1.7994, 1.798, 1.7672, 1.7671, 1.767, 1.7665, 1.7665, 1.7662, 1.765, 1.7536, 1.7497, 1.743, 1.733, 1.717, 1.7153, 1.7144, 1.7144, 1.7142, 1.7139, 1.7137, 1.7135, 1.7134, 1.7131, 1.713, 1.7125, 1.7114, 1.7101, 1.6851, 1.6729, 1.542, 1.6272, 1.5509, 1.2872, 1.1262, 1.4565, 1.504, 0.6844, 1.5777, 1.5776, 1.0606, 1.2999, 1.3856, 0.8308, 0.8128, 0.7613, 0.6113, 0.4055, 0.938, 0.3878, 0.4681, -0.1453, 0.282, 0.4533, 0.0351, 0.0681, 0.7524, 0.6705, -0.2483, 0.0886, -0.4294, 0.703, 0.2845, 0.56, -0.3479, -0.0062, -0.0923, 0.4073, -0.2851, 0.1449, -0.5101, -0.6455, -0.7173]}, \"token.table\": {\"Topic\": [2, 5, 1, 1, 3, 5, 2, 1, 2, 5, 1, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 5, 3, 1, 1, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 3, 4, 1, 1, 2, 3, 4, 5, 3, 5, 4, 1, 2, 3, 1, 2, 3, 1, 3, 3, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 3, 5, 4, 1, 2, 3, 4, 5, 5, 4, 1, 1, 3, 2, 3, 4, 1, 3, 1, 3, 4, 1, 3, 4, 5, 4, 5, 5, 1, 1, 3, 4, 5, 4, 1, 2, 4, 3, 5, 5, 2, 1, 2, 3, 4, 5, 5, 3, 2, 3, 5, 1, 2, 3, 4, 5, 3, 1, 1, 2, 3, 5, 1, 4, 3, 2, 2, 1, 1, 1, 3, 4, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 3, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 5, 4, 1, 1, 3, 4, 5, 1, 1, 2, 3, 4, 2, 3, 4, 2, 4, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 2, 4, 5, 5, 1, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 3, 1, 3, 1, 2, 3, 4, 5, 3, 4, 2, 3, 5, 1, 5, 4, 1, 2, 3, 4, 5, 3, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 2, 4, 5, 1, 2, 4, 5, 1, 1, 2, 3, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 3, 5, 4, 4, 5, 1, 2, 3, 4, 5, 3, 5, 2, 4, 5, 4, 4, 1, 2, 3, 4, 5, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 4, 4, 1, 2, 3, 4, 5, 2, 4, 3, 1, 4, 5, 5, 3, 1, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 5, 4, 1, 2, 4, 1, 1, 2, 3, 5, 1, 1, 3, 2, 2, 4, 5, 1, 2, 3, 4, 5, 3, 1, 1, 2, 3, 4, 5, 2, 5, 3, 2, 3, 4, 4, 1, 2, 4, 1, 2, 3, 4, 5, 1, 4, 1, 1, 2, 3, 4, 5, 5, 4, 1, 2, 3, 4, 5, 2, 1, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 2, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 4, 2, 4, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 4, 3, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 3, 1, 2, 3, 4, 1, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5], \"Freq\": [0.7921001975649546, 0.2263143421614156, 0.8134847856180343, 0.814076952243264, 0.135679492040544, 0.8166514005442236, 0.8144862642788951, 0.9095666607526943, 0.10106296230585492, 0.8165733381037117, 0.721215846552423, 0.10303083522177471, 0.10303083522177471, 0.051515417610887355, 0.8960330593428817, 0.08145755084935288, 0.35103667064828187, 0.1170122235494273, 0.07800814903295153, 0.19502037258237884, 0.3120325961318061, 0.9097235156560408, 0.7797814700062972, 0.8127333384558062, 0.3509817020188553, 0.5849695033647588, 0.22555333726982252, 0.37055191122899417, 0.177220479283432, 0.14499857395917162, 0.088610239641716, 0.6494171296390684, 0.025949333711926037, 0.25949333711926037, 0.23354400340733433, 0.25949333711926037, 0.2075946696954083, 0.9429863216823178, 0.04870719854343114, 0.340950389804018, 0.43836478689088026, 0.14612159563029342, 0.6780004160174452, 0.9429813476083023, 0.19870702118683292, 0.08516015193721412, 0.283867173124047, 0.1419335865620235, 0.31225389043645174, 0.8675184602080306, 0.09220772219039028, 0.5993501942375369, 0.15367953698398382, 0.10757567588878866, 0.04610386109519514, 0.18816691169208516, 0.7526676467683406, 0.942873573302951, 0.2634152605200171, 0.22978778045363196, 0.14011450027660485, 0.33067022065278745, 0.03923206007744936, 0.7805185778613936, 0.815812171691693, 0.7906619052418667, 0.21141182351424995, 0.21141182351424995, 0.5285295587856249, 0.09348796826982254, 0.8413917144284029, 0.7793441063387205, 0.3187286251886336, 0.7012029754149939, 0.8187378295793597, 0.16374756591587195, 0.8144116232304264, 0.36992077574491683, 0.15232031942437751, 0.2828805932167011, 0.1305602737923236, 0.0652801368961618, 0.31475370314553974, 0.2360652773591548, 0.2950815966989435, 0.07868842578638494, 0.07868842578638494, 0.8300476337874709, 0.11857823339821012, 0.6779730592359127, 0.8714209707607641, 0.7799466934154834, 0.0654749324987565, 0.6547493249875649, 0.130949864997513, 0.130949864997513, 0.1456272149534611, 0.4951325308417677, 0.17475265794415332, 0.08737632897207666, 0.08737632897207666, 0.4628999327378285, 0.4959642136476734, 0.8161704573605826, 0.6578068797952286, 0.19797829421649782, 0.32525005478424646, 0.14141306729749845, 0.11313045383799876, 0.21211960094624768, 0.9096330097394199, 0.6577279587261841, 0.8110439750191621, 0.885148999058267, 0.11064362488228338, 0.08742061606799649, 0.17484123213599298, 0.6993649285439719, 0.8805293799925186, 0.7798794253314622, 0.2858641443963162, 0.21439810829723713, 0.5002622526935533, 0.7524639969133499, 0.10749485670190713, 0.053747428350953565, 0.10749485670190713, 0.7906514419758898, 0.6779218078898372, 0.8161266281656328, 0.7313833842686923, 0.7519490621743046, 0.05371064729816462, 0.10742129459632924, 0.10742129459632924, 0.6577293685586495, 0.8113397640601414, 0.10141747050751768, 0.10141747050751768, 0.7799118002894379, 0.8165449850339399, 0.6778377466565338, 0.7343869297281422, 0.07324096454853961, 0.3295843404684282, 0.183102411371349, 0.366204822742698, 0.07324096454853961, 0.9093815187116631, 0.7799295660159237, 0.18387610508705093, 0.7355044203482037, 0.6778985307875594, 0.23484288240619622, 0.058710720601549055, 0.11742144120309811, 0.5283964854139415, 0.058710720601549055, 0.8670161419771502, 0.867562404630123, 0.21096579458313744, 0.21096579458313744, 0.49225352069398737, 0.07032193152771248, 0.8806438558113197, 0.6577639566081425, 0.8659970177051207, 0.8717533241701723, 0.8712359943407423, 0.8677756885214488, 0.811821209998499, 0.6804255397729845, 0.1601001270054081, 0.1601001270054081, 0.6577579270709638, 0.43996421809179886, 0.048884913121310984, 0.09776982624262197, 0.3910793049704879, 0.048884913121310984, 0.8110159074880957, 0.19577557550193975, 0.3132409208031036, 0.17619801795174578, 0.13704290285135784, 0.19577557550193975, 0.7796716876659767, 0.7435334837392522, 0.13518790613440948, 0.06759395306720474, 0.7002599436359728, 0.04119176139035134, 0.12357528417105403, 0.12357528417105403, 0.8126294246033451, 0.8143456333111435, 0.7799714449418197, 0.038387358543815774, 0.23032415126289466, 0.23032415126289466, 0.19193679271907887, 0.26871150980671044, 0.21760367262611294, 0.30662335688225006, 0.1483661404268952, 0.21760367262611294, 0.10880183631305647, 0.6780482593467551, 0.026058093614002447, 0.6514523403500612, 0.07817428084200735, 0.13029046807001224, 0.13029046807001224, 0.6973259974009668, 0.04226218166066465, 0.12678654498199396, 0.12256032681592749, 0.012678654498199396, 0.7270097008628904, 0.05592382314329926, 0.1677714694298978, 0.05592382314329926, 0.6431623969107935, 0.32158119845539673, 0.03573124427282186, 0.2773846849683306, 0.1951966301628993, 0.2054701370135782, 0.17464961646154148, 0.15410260276018364, 0.22287947057263255, 0.24764385619181392, 0.11143973528631627, 0.2724082418109953, 0.13620412090549766, 0.6238985450058521, 0.20796618166861736, 0.12477970900117041, 0.04159323633372347, 0.7805620398222595, 0.12613011234086294, 0.30631598711352426, 0.2162230497271936, 0.19820446224992747, 0.1621672872953952, 0.7343021220591907, 0.6577716190844267, 0.5867747998751328, 0.05867747998751328, 0.05867747998751328, 0.26404865994380977, 0.02933873999375664, 0.816535714224485, 0.9430146379032854, 0.8110775323281617, 0.7855304772530054, 0.04909565482831284, 0.09819130965662567, 0.04909565482831284, 0.7314011269246308, 0.11546235026366054, 0.6927741015819633, 0.03848745008788685, 0.1539498003515474, 0.8718998464071805, 0.17588041853426592, 0.7035216741370637, 0.734328404572717, 0.6577130307472522, 0.27542371761858164, 0.22951976468215135, 0.09180790587286054, 0.04590395293643027, 0.36723162349144217, 0.8846978645069156, 0.7343296104318091, 0.14120602085499517, 0.24206746432284884, 0.1613783095485659, 0.24206746432284884, 0.2218951756292781, 0.9429755300302649, 0.14215029037426333, 0.14215029037426333, 0.7107514518713167, 0.6577198214541613, 0.10685017775102926, 0.36329060435349947, 0.14959024885144095, 0.10685017775102926, 0.25644042660247024, 0.23706757756731575, 0.11853378878365788, 0.30818785083751044, 0.23706757756731575, 0.0948270310269263, 0.7799660878241711, 0.2406331650767919, 0.37020486934891056, 0.09255121733722764, 0.20361267814190082, 0.11106146080467318, 0.8114268835857574, 0.7906858278864961, 0.697492282599391, 0.06974922825993911, 0.11624871376656518, 0.09299897101325214, 0.023249742753313034, 0.8148840955381963, 0.19409568944307734, 0.7763827577723094, 0.816641012827831, 0.13522898651623666, 0.81137391909742, 0.9469872360445521, 0.1206534195800005, 0.6032670979000025, 0.1206534195800005, 0.19220966513445462, 0.09610483256722731, 0.14415724885084094, 0.4805241628361365, 0.09610483256722731, 0.1461532175870402, 0.2923064351740804, 0.4384596527611206, 0.09743547839136013, 0.8714870898551902, 0.2146850459285306, 0.07156168197617686, 0.07156168197617686, 0.5009317738332381, 0.07156168197617686, 0.7797493835643114, 0.7672557696950619, 0.15345115393901237, 0.07189358160721304, 0.14378716321442608, 0.5751486528577043, 0.14378716321442608, 0.07189358160721304, 0.7826595204865747, 0.13044325341442914, 0.7618437659411262, 0.09523047074264078, 0.09523047074264078, 0.16804653390684993, 0.6721861356273997, 0.8793822847871304, 0.13957127304853892, 0.27914254609707784, 0.13957127304853892, 0.17446409131067367, 0.27914254609707784, 0.975995658894081, 0.8713033894876272, 0.7797520880528341, 0.15187251593684967, 0.30374503187369933, 0.455617547810549, 0.10124834395789978, 0.0502031856696457, 0.30121911401787416, 0.15060955700893708, 0.1004063713392914, 0.4016254853571656, 0.2700624556491289, 0.13503122782456445, 0.5401249112982578, 0.8678649877435023, 0.9430439515888576, 0.13828364267897936, 0.5531345707159174, 0.17779325487297346, 0.07901922438798821, 0.059264418290991154, 0.8143477176664707, 0.6577924954692365, 0.815737788852803, 0.33154178479450763, 0.09472622422700218, 0.5683573453620131, 0.04736311211350109, 0.8677165193493367, 0.8364477045960232, 0.0760407004178203, 0.0760407004178203, 0.12253737584877639, 0.306343439621941, 0.18380606377316458, 0.1531717198109705, 0.24507475169755277, 0.6780792080643947, 0.20196094368223597, 0.3454595089301405, 0.15944285027544947, 0.1753871353029944, 0.12223951854451125, 0.3872956648526448, 0.30122996155205706, 0.08606570330058774, 0.1290985549508816, 0.10758212912573467, 0.43125852273366005, 0.02395880681853667, 0.2635468750039034, 0.2395880681853667, 0.04791761363707334, 0.5711628298328564, 0.02719822999204078, 0.21758583993632624, 0.19038760994428544, 0.02719822999204078, 0.7314289185587083, 0.05933895138888431, 0.2076863298610951, 0.35603370833330583, 0.05933895138888431, 0.32636423263886366, 0.17963396823486266, 0.34489721901093634, 0.15089253331728464, 0.25148755552880775, 0.07185358729394507, 0.6577358929596189, 0.32153459378007104, 0.5626855391151243, 0.08038364844501776, 0.2019136307349243, 0.3854714768575827, 0.1284904922858609, 0.055067353836797533, 0.22026941534719013, 0.1812054948335468, 0.28690870015311576, 0.1359041211251601, 0.12080366322236454, 0.2718082422503202, 0.7854398155975975, 0.08727109062195527, 0.27539146409451287, 0.6425800828871967, 0.09179715469817097, 0.7893787108174951, 0.8794441074919163, 0.8165908763697761, 0.14198141851514778, 0.3407554044363547, 0.05679256740605912, 0.3123591207333251, 0.17037770221817736, 0.22526289322373247, 0.6757886796711974, 0.7343655513197187, 0.33875616270013204, 0.6775123254002641, 0.6577410966819212, 0.7905807003317513, 0.05146787675977029, 0.07720181513965543, 0.25733938379885146, 0.4632108908379326, 0.12866969189942573, 0.1492454468408605, 0.1492454468408605, 0.596981787363442, 0.1263421453268531, 0.4211404844228436, 0.23162726643256398, 0.14739916954799526, 0.08422809688456871, 0.15123586440735157, 0.09074151864441095, 0.2117302101702922, 0.3629660745776438, 0.1814830372888219, 0.2869162294974397, 0.35598865511719374, 0.17002443229477912, 0.15408464176714357, 0.03719284456448293, 0.7314001696825579, 0.10782578031526387, 0.7008675720492151, 0.05391289015763193, 0.05391289015763193, 0.10782578031526387, 0.2692992252197537, 0.2971577657597282, 0.17643742341983862, 0.11143416215989807, 0.1485788828798641, 0.2226983269581496, 0.1113491634790748, 0.6680949808744487, 0.909709371340358, 0.6779070985962939, 0.1384058576528783, 0.19376820071402961, 0.1384058576528783, 0.22144937224460526, 0.30449288683633224, 0.18301523768338285, 0.3879923038887716, 0.19033584719071814, 0.1683740186687122, 0.07320609507335313, 0.7993296449364625, 0.2179989940735807, 0.657754871482214, 0.19648284565527038, 0.23577941478632447, 0.19648284565527038, 0.27507598391737853, 0.11788970739316224, 0.8713346529862009, 0.6577793250825185, 0.9288409722564869, 0.7800314733199775, 0.04588420431293985, 0.13765261293881956, 0.6778874366374902, 0.9760058772522903, 0.7674507672227096, 0.07674507672227095, 0.07674507672227095, 0.6577654468579428, 0.13987239169070853, 0.06993619584535427, 0.13987239169070853, 0.13987239169070853, 0.4895533709174799, 0.2620698713305995, 0.13103493566529975, 0.524139742661199, 0.6577275262614751, 0.6759861428935753, 0.12674740179254537, 0.12674740179254537, 0.8117967291255084, 0.18430099169758152, 0.09215049584879076, 0.5529029750927446, 0.18430099169758152, 0.9100095239806267, 0.867639196605616, 0.779755063729442, 0.7342335578043347, 0.8142784364048288, 0.829933175764234, 0.1659866351528468, 0.2503769220423405, 0.39121394069115706, 0.14083701864881654, 0.10953990339352397, 0.10953990339352397, 0.813498534293842, 0.8842244385202811, 0.06091548013197322, 0.30457740065986605, 0.3959506208578259, 0.21320418046190626, 0.06091548013197322, 0.9738429927663949, 0.9097126499536013, 0.7795979863990915, 0.8192595650172332, 0.13654326083620552, 0.6577641459384355, 0.7906427930750471, 0.11145138651244406, 0.7801597055871085, 0.11145138651244406, 0.9070785378032447, 0.004319421608586879, 0.0431942160858688, 0.0431942160858688, 0.004319421608586879, 0.8047555560151496, 0.15328677257431422, 0.9131862708416368, 0.1740893514949983, 0.2959518975414971, 0.20890722179399795, 0.2263161569434978, 0.10445361089699898, 0.6777902735052359, 0.7914011681972016, 0.10921210251594038, 0.3458383246338112, 0.21842420503188076, 0.2002221879458907, 0.12741411960193044, 0.7343120666504985, 0.9538823471517482, 0.7799194535806622, 0.7409034781494844, 0.29636139125979377, 0.02957047629946505, 0.6801209548876961, 0.14785238149732524, 0.0591409525989301, 0.0591409525989301, 0.22525999283305045, 0.6757799784991513, 0.10046157585058996, 0.10046157585058996, 0.10046157585058996, 0.6027694551035397, 0.10046157585058996, 0.30422779507179537, 0.20281853004786357, 0.507046325119659, 0.20695922922637477, 0.2660904375767676, 0.32522164592716035, 0.0886968125255892, 0.11826241670078559, 0.5343447807194783, 0.29388962939571306, 0.10686895614389566, 0.05343447807194783, 0.16303301157816977, 0.8151650578908488, 0.7343173295041913, 0.7906089446666029, 0.8594715765245236, 0.12278165378921765, 0.67791617130748, 0.27348653398118905, 0.2119520638354215, 0.13674326699059453, 0.27348653398118905, 0.10939461359247561, 0.6459619940898209, 0.05383016617415174, 0.18840558160953108, 0.08074524926122761, 0.02691508308707587, 0.682461472799463, 0.18612585621803537, 0.1240839041453569, 0.10181045441431721, 0.8144836353145377, 0.10181045441431721, 0.8669858469039688, 0.814343056857904, 0.11700748236427912, 0.39002494121426373, 0.23401496472855823, 0.11700748236427912, 0.11700748236427912, 0.11622727178125422, 0.30219090663126097, 0.18596363485000678, 0.27894545227501016, 0.11622727178125422, 0.1702439766591809, 0.6809759066367236, 0.7799158758580174, 0.7570430153610304, 0.031543458973376265, 0.031543458973376265, 0.15771729486688132, 0.8676489162992348, 0.20665938143674098, 0.20665938143674098, 0.1653275051493928, 0.45465063916083015, 0.7797833146375455, 0.15256215508599197, 0.2929193377651046, 0.15866464128943164, 0.30512431017198394, 0.09153729305159518, 0.677965714071211, 0.08379214913797621, 0.27930716379325404, 0.3072378801725794, 0.19551501465527782, 0.13965358189662702, 0.912237282573122, 0.0845119868856201, 0.1690239737712402, 0.3380479475424804, 0.22536529836165362, 0.1690239737712402, 0.2261226802387532, 0.02826533502984415, 0.6501027056864154, 0.08479600508953244, 0.1955026075797316, 0.32118285530955903, 0.22343155151969324, 0.06982235984990413, 0.18153813560975077, 0.8144604281782858, 0.4975750140637933, 0.1298021775818591, 0.04326739252728637, 0.2596043551637182, 0.06490108879092955, 0.29508147001289536, 0.3245896170141849, 0.20655702900902675, 0.10327851450451338, 0.08852444100386861, 0.7342800893151433, 0.47495303242148845, 0.17498269615528522, 0.17498269615528522, 0.07499258406655081, 0.0999901120887344, 0.6779328212151073, 0.8147230201205821, 0.1413249604927025, 0.33700567502105977, 0.20655186533548825, 0.18480956372122634, 0.13045380968557152, 0.38133543695133415, 0.11440063108540025, 0.17795653724395594, 0.17795653724395594, 0.15253417478053366, 0.37781012637585937, 0.30652519687098023, 0.10692739425731869, 0.10692739425731869, 0.09979890130683078, 0.4782756479557358, 0.2152240415800811, 0.11956891198893395, 0.16739647678450753, 0.02391378239778679, 0.9294172619568197, 0.22676033588553143, 0.16708656328407578, 0.27449935396669595, 0.23869509040582254, 0.08354328164203789, 0.14081550287331984, 0.6414928464229015, 0.1251693358873954, 0.03129233397184885, 0.0625846679436977, 0.18458169331549268, 0.3123690194569876, 0.12778732614149493, 0.18458169331549268, 0.21297887690249154, 0.6778563351192826, 0.8166473236761765], \"Term\": [\"able\", \"able\", \"agency\", \"alien\", \"alien\", \"alma\", \"amazon\", \"analysis\", \"analysis\", \"anatomy\", \"animal\", \"animal\", \"animal\", \"animal\", \"anthony\", \"anthony\", \"art\", \"art\", \"art\", \"art\", \"art\", \"artemis\", \"astronautics\", \"astronomy\", \"australian\", \"australian\", \"author\", \"author\", \"author\", \"author\", \"author\", \"average\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bard\", \"battle\", \"battle\", \"battle\", \"battle\", \"bedroom\", \"beedle\", \"begin\", \"begin\", \"begin\", \"begin\", \"begin\", \"beryl\", \"bestselling\", \"bestselling\", \"bestselling\", \"bestselling\", \"bestselling\", \"bibliography\", \"bibliography\", \"biology\", \"book\", \"book\", \"book\", \"book\", \"book\", \"bourne\", \"boyfriend\", \"bradford\", \"break\", \"break\", \"break\", \"bridgerton\", \"bridgerton\", \"brutus\", \"c\", \"c\", \"caesar\", \"caesar\", \"casino\", \"century\", \"century\", \"century\", \"century\", \"century\", \"change\", \"change\", \"change\", \"change\", \"change\", \"charlotte\", \"charlotte\", \"charmain\", \"chauveau\", \"china\", \"choice\", \"choice\", \"choice\", \"choice\", \"city\", \"city\", \"city\", \"city\", \"city\", \"climate\", \"climate\", \"code\", \"colleen\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comet\", \"commits\", \"computing\", \"consciousness\", \"consciousness\", \"contains\", \"contains\", \"contains\", \"coop\", \"cove\", \"critical\", \"critical\", \"critical\", \"cultural\", \"cultural\", \"cultural\", \"cultural\", \"cyberspace\", \"dallas\", \"darryl\", \"data\", \"de\", \"de\", \"de\", \"de\", \"deathly\", \"debate\", \"debate\", \"debate\", \"della\", \"derek\", \"deserve\", \"determination\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined\", \"devin\", \"dexie\", \"dinner\", \"dinner\", \"dior\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dub\", \"dune\", \"early\", \"early\", \"early\", \"early\", \"ecofeminist\", \"effective\", \"eleanor\", \"elena\", \"elzbieta\", \"engage\", \"engineering\", \"environmental\", \"environmental\", \"environmental\", \"environmentality\", \"essay\", \"essay\", \"essay\", \"essay\", \"essay\", \"et\", \"even\", \"even\", \"even\", \"even\", \"even\", \"expedition\", \"explore\", \"explore\", \"explore\", \"explores\", \"explores\", \"explores\", \"explores\", \"extraterrestrial\", \"ezra\", \"faerie\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fantasyroman\", \"father\", \"father\", \"father\", \"father\", \"father\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"field\", \"field\", \"field\", \"field\", \"film\", \"film\", \"film\", \"find\", \"find\", \"find\", \"find\", \"find\", \"first\", \"first\", \"first\", \"first\", \"first\", \"form\", \"form\", \"form\", \"form\", \"fragile\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"frog\", \"frontier\", \"future\", \"future\", \"future\", \"future\", \"future\", \"gabby\", \"gansett\", \"gap\", \"genre\", \"genre\", \"genre\", \"genre\", \"godmother\", \"good\", \"good\", \"good\", \"good\", \"goodreads\", \"gothic\", \"gothic\", \"grandparent\", \"hallows\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hank\", \"hawke\", \"help\", \"help\", \"help\", \"help\", \"help\", \"helped\", \"henry\", \"henry\", \"henry\", \"hermione\", \"historical\", \"historical\", \"historical\", \"historical\", \"historical\", \"history\", \"history\", \"history\", \"history\", \"history\", \"holocaust\", \"home\", \"home\", \"home\", \"home\", \"home\", \"honoree\", \"hoover\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hussite\", \"ice\", \"ice\", \"ichimei\", \"icy\", \"icy\", \"ida\", \"illustration\", \"illustration\", \"illustration\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"international\", \"international\", \"international\", \"international\", \"irvel\", \"island\", \"island\", \"island\", \"island\", \"island\", \"isobelle\", \"issue\", \"issue\", \"james\", \"james\", \"james\", \"james\", \"james\", \"jeffrey\", \"jeffrey\", \"jonathan\", \"jonathan\", \"jonathan\", \"joseph\", \"joseph\", \"joubert\", \"journey\", \"journey\", \"journey\", \"journey\", \"journey\", \"julian\", \"juliette\", \"kanas\", \"keep\", \"keep\", \"keep\", \"keep\", \"king\", \"king\", \"king\", \"king\", \"king\", \"kingdom\", \"kingdom\", \"kingdom\", \"kinkley\", \"kirsten\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lake\", \"lakota\", \"langdon\", \"language\", \"language\", \"language\", \"language\", \"lawrence\", \"le\", \"le\", \"le\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"legible\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"literary\", \"literary\", \"literary\", \"literary\", \"literary\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"logan\", \"london\", \"london\", \"london\", \"london\", \"london\", \"love\", \"love\", \"love\", \"love\", \"love\", \"l\\u00e9o\", \"machine\", \"machine\", \"machine\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"man\", \"mar\", \"mar\", \"mary\", \"mary\", \"mary\", \"mattie\", \"mccarthy\", \"mcgee\", \"meet\", \"meet\", \"meet\", \"meet\", \"meet\", \"melanie\", \"melanie\", \"meredith\", \"monster\", \"monster\", \"montverre\", \"morse\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"museum\", \"museum\", \"museum\", \"must\", \"must\", \"must\", \"must\", \"must\", \"mystery\", \"mystery\", \"mystery\", \"mystery\", \"mystery\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nonhuman\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"odd\", \"odd\", \"odd\", \"odysseus\", \"oil\", \"old\", \"old\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"one\", \"one\", \"oz\", \"oz\", \"pageturner\", \"past\", \"past\", \"past\", \"past\", \"past\", \"pastor\", \"penn\", \"pharaoh\", \"philosophy\", \"philosophy\", \"philosophy\", \"phoebe\", \"piper\", \"planet\", \"planet\", \"planet\", \"playful\", \"police\", \"police\", \"police\", \"police\", \"police\", \"poor\", \"poor\", \"poor\", \"popculture\", \"popular\", \"popular\", \"popular\", \"portrayed\", \"prince\", \"prince\", \"prince\", \"prince\", \"prototype\", \"pseudoscience\", \"psychiatry\", \"purchase\", \"rachel\", \"ranch\", \"ranch\", \"reader\", \"reader\", \"reader\", \"reader\", \"reader\", \"rebus\", \"religion\", \"return\", \"return\", \"return\", \"return\", \"return\", \"reviewer\", \"revision\", \"rivalry\", \"rose\", \"rose\", \"rumm\", \"rylan\", \"sam\", \"sam\", \"sam\", \"science\", \"science\", \"science\", \"science\", \"science\", \"scientific\", \"scientific\", \"scifi\", \"secret\", \"secret\", \"secret\", \"secret\", \"secret\", \"security\", \"selection\", \"series\", \"series\", \"series\", \"series\", \"series\", \"seventh\", \"sf\", \"shakespeare\", \"shepard\", \"shepard\", \"shes\", \"shes\", \"shes\", \"shes\", \"shes\", \"shop\", \"shop\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"single\", \"single\", \"single\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"space\", \"space\", \"space\", \"space\", \"specific\", \"specific\", \"spell\", \"stem\", \"stephen\", \"stephen\", \"stokes\", \"story\", \"story\", \"story\", \"story\", \"story\", \"study\", \"study\", \"study\", \"study\", \"study\", \"summer\", \"summer\", \"summer\", \"sunny\", \"sunny\", \"sunny\", \"sunset\", \"swipe\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tale\", \"tale\", \"tale\", \"tale\", \"tale\", \"tapestry\", \"tapestry\", \"tariq\", \"technology\", \"technology\", \"technology\", \"technology\", \"television\", \"text\", \"text\", \"text\", \"text\", \"therapy\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tina\", \"together\", \"together\", \"together\", \"together\", \"together\", \"tower\", \"town\", \"town\", \"town\", \"town\", \"town\", \"travel\", \"travel\", \"travel\", \"travel\", \"two\", \"two\", \"two\", \"two\", \"two\", \"twobits\", \"u\", \"u\", \"u\", \"u\", \"u\", \"war\", \"war\", \"war\", \"war\", \"war\", \"warsaw\", \"well\", \"well\", \"well\", \"well\", \"well\", \"winniethepooh\", \"witcher\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"writer\", \"writer\", \"writer\", \"writer\", \"writer\", \"ya\", \"year\", \"year\", \"year\", \"year\", \"year\", \"york\", \"york\", \"york\", \"york\", \"york\", \"young\", \"young\", \"young\", \"young\", \"young\", \"yvette\", \"zack\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 4, 3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el3046819935313920806394267984\", ldavis_el3046819935313920806394267984_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el3046819935313920806394267984\", ldavis_el3046819935313920806394267984_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el3046819935313920806394267984\", ldavis_el3046819935313920806394267984_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
